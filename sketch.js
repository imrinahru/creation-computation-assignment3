/*

Project Title: When My Favorite Things Grew a Personality
Team member: Rina Chen, Amelia Lochhead, Niloofar Sanandajizadeh
Helpful Code Examples:
1. Detect facial landmarks and use the detected value to control a servo:
   https://editor.p5js.org/creationcomputation/sketches/Hg2hw9pcA
2. Use ML5 body pose detection with MoveNet to control a servo:
   https://editor.p5js.org/npuckett/sketches/hMLZ2Xmzg

Overview:
 - The concept is for the motor to point to the nose position of the viewer, but once it is detected that the viewer is approaching the camera (through measuring changing distance between eyes), the servo quickly shifts away from the viewer.
 - Measure eye distance (FaceMesh landmarks 159 & 386) each frame.
 - Smooth eye distance (lerp) to reduce noise.
 - Compute how the eye distance changed (delta) and smooth that too.
 - Convert the smoothed delta into an escapeFactor (0..1) using lower & upper thresholds.
     * escapeFactor = 0  -> pure tracking (servo maps nose X directly)
     * escapeFactor = 1  -> full escape (servo uses inverted mapping)
     * trackAngle controls the angle of servo during its tracking mode; escapeAngle controls the angle of servo during its escaping mode
     * values in between produce a blended angle with lerp(trackAngle, escapeAngle, escapeFactor)
 - Smooth the final servo angle before sending to Arduino.

Key Improvements:
 - In total, used lerp in four different occassions to achieve smooth/nuanced movement
 - Avoided abrupt mode switching with continuous blending, using lerp.
 - Added initial warm-up smoothing after detecting eyedistance (gentle for first frames) based on AI suggestion (ChatGPT-5, 2025-11-13)
 - Added threshold range for gradual response, based on AI suggestion (ChatGPT-5, 2025-11-13).

*/

const SERVO_MIN = 0;
const SERVO_MAX = 180;

// -------- Smoothing & Behavior Configuration --------
const eyeDistanceLerpAmt = 0.5;      // Normal smoothing for eye distance after warm-up
const initialEyeDistanceLerpAmt = 0.2; // Gentler (slower) smoothing for first few frames
const warmupFrames = 15;              // Frames to use initial smoothing amount

const angleLerpAmt = 0.2;            // Smoothing for servo angle (noseTargetAngle -> smoothedNoseAngle) Smaller the smoother

// Delta blending thresholds
const lowerDeltaThreshold = 0.7;        // Delta at which escape begins to influence angle
const upperDeltaThreshold = 2;       // Delta at which escapeFactor reaches 1 (full escape)


// Serial sending constraints
const minAngleChangeToSend = 0.5;     // Only send if angle changed this much
const sendEveryNFrames = 1;           // Send servo data every N frames

// Validation bounds (filter obviously bad eye distances)
const minValidEyeDistance = 10;
const maxValidEyeDistance = 600;

// -------- Globals --------
let video;
let bodyPose;
let faceMesh;

let poses = [];
let faces = [];

let confidenceThreshold = 0.2;
let showVideo = true;
let port;
let connectBtn;

// Eye distance tracking
let previousEyeDistance = null; // Smoothed eye distance from prior frame(s)
let smoothedDelta = 0;          // Smoothed change (delta) in eye distance
let framesSinceFirstEye = 0;    // How many frames since first valid eye distance

// Servo angles
let noseTargetAngle = 90;       // Immediate blended mapping result
let smoothedNoseAngle = 90;     // Further smoothed angle actually sent

// For display/debug
let escapeFactor = 0;           // 0..1 blending value



function preload() {
  // MoveNet BodyPose currently detects a single person (single-pose), and faceMesh can be delimited to a single person
  bodyPose = ml5.bodyPose("MoveNet", { flipped: true });
  faceMesh = ml5.faceMesh({
    maxFaces: 1,
    refineLandmarks: false,
    flipped: true,
  });
}

function setup() {
  createCanvas(640, 480);
  angleMode(DEGREES);

  //flipHorizontal/flipped may not be working here (?)
  video = createCapture({
    video: { width: 640, height: 480, flipHorizontal: true }
  });
  video.size(640, 480);
  video.hide();

  // Start detection
  bodyPose.detectStart(video, gotPoses);
  faceMesh.detectStart(video, gotFaces);

  // Serial
  initializeSerial();
  createConnectButton();
}

function draw() {
  background(220);

  // Mirrored video display (Code generated by ChatGPT5, 2025-11-15)
  if (showVideo) {
    push();
    translate(width, 0);
    scale(-1, 1);
    image(video, 0, 0, width, height);
    pop();
  }

  // Get keypoints
  const noseKP     = getBodyKeypoint(0);
  const rightEyeKP = getFaceKeypoint(159);
  const leftEyeKP  = getFaceKeypoint(386);

  // Compute current eye distance if both eye landmarks exist
  let currentEyeDistance = null;
  if (rightEyeKP && leftEyeKP) {
    currentEyeDistance = dist(
      rightEyeKP.x, rightEyeKP.y,
      leftEyeKP.x, leftEyeKP.y
    );
    // Filter out improbable values (noise/outliers)
    if (currentEyeDistance < minValidEyeDistance ||
        currentEyeDistance > maxValidEyeDistance) {
      currentEyeDistance = null;
    }
  }

  // Only proceed if nose and a valid eye distance are available
  if (noseKP && noseKP.confidence > confidenceThreshold && currentEyeDistance !== null) {

    // Initialize or update smoothed eye distance
    if (previousEyeDistance === null) {
      previousEyeDistance = currentEyeDistance;
      framesSinceFirstEye = 0;
    } else {
      framesSinceFirstEye++;
      // Use gentle smoothing early, then normal smoothing
      const amt = (framesSinceFirstEye < warmupFrames) ? initialEyeDistanceLerpAmt : eyeDistanceLerpAmt;
      previousEyeDistance = lerp(previousEyeDistance, currentEyeDistance, amt);
    }

    // Raw delta: how much the distance changed this frame
    const rawDelta = currentEyeDistance - previousEyeDistance;

    // Smooth the delta itself to help reject noise spikes
    smoothedDelta = lerp(smoothedDelta, rawDelta, 0.5);

    // Convert smoothedDelta to escapeFactor in [0,1]
    // (delta - lower) / (upper - lower) then clamp (Code generated from ChatGPT5, 2025-11-15)
    escapeFactor = (smoothedDelta - lowerDeltaThreshold) / (upperDeltaThreshold - lowerDeltaThreshold);
    escapeFactor = constrain(escapeFactor, 0, 1);


    // Compute both mapping angles
    const trackAngle  = mapToServoTrack(noseKP.x, SERVO_MIN, SERVO_MAX);
    const escapeAngle = mapToServoEscape(noseKP.x, SERVO_MIN, SERVO_MAX);

    // Blend between them based on escapeFactor
    noseTargetAngle = lerp(trackAngle, escapeAngle, escapeFactor);
  }

  // Smooth the servo angle itself to avoid jittery small changes
  smoothedNoseAngle = lerp(smoothedNoseAngle, noseTargetAngle, angleLerpAmt);


  // Draw nose helper if available
  if (noseKP && noseKP.confidence > confidenceThreshold) {
    drawPulsingTarget(noseKP.x, noseKP.y, color(138, 43, 226));
  }

  // Send to Arduino if:
  //  - Serial is open
  //  - Proper frame interval
  //  - Angle changed enough to matter
  if (port && typeof port.opened === "function" && port.opened()) {
    if (frameCount % sendEveryNFrames === 0 &&
        abs(smoothedNoseAngle - noseTargetAngle) >= minAngleChangeToSend) {
      sendDataToArduino(smoothedNoseAngle);
    }
  }

  // Debug / info overlay
  displayInfo(currentEyeDistance);
  updateConnectButton();
}

// ---------- Mapping Functions ----------
function mapToServoTrack(xPos, sMin, sMax) {
  const v = map(xPos, 0, width, sMin, sMax);
  return constrain(v, Math.min(sMin, sMax), Math.max(sMin, sMax));
}

function mapToServoEscape(xPos, sMin, sMax) {
  const normalized = xPos / width; // 0..1
  // If nose on left half, go to right extreme; if nose on right half, go to left extreme
  let inv = (normalized < 0.5) ? sMax : sMin;
  return inv;

}

// ---------- Serial ----------
function sendDataToArduino(angle) {
  const line = String(round(angle)) + '\n';
  try {
    port.write(line);
  } catch (e) {
    // Ignore write errors
  }
}

// ---------- UI / Display ----------
function displayInfo(currentEyeDistance) {
  fill(0);
  noStroke();
  textAlign(LEFT, TOP);
  textSize(14);

  text(`Servo range: ${SERVO_MIN}째 - ${SERVO_MAX}째`, 10, 10);
  text(`Escape factor: ${nf(escapeFactor, 0, 2)} (0=track,1=escape)`, 10, 30);

  if (currentEyeDistance !== null) {
    text(`Eye dist (current): ${nf(currentEyeDistance, 0, 2)} px`, 10, 50);
    text(`Eye dist (smoothed): ${nf(previousEyeDistance || 0, 0, 2)} px`, 10, 70);
    text(`Delta (smoothed): ${nf(smoothedDelta, 0, 2)} px`, 10, 90);
  } else {
    text(`Eye dist: (not detected)`, 10, 50);
  }

  text(`Track angle (raw target): ${nf(noseTargetAngle, 0, 2)}째`, 10, 110);
  text(`Servo angle (smoothed): ${nf(smoothedNoseAngle, 0, 2)}째`, 10, 130);
  text("SPACE toggles video", 10, 160);
  text("Landmarks: nose(0), eyes(159,386)", 10, 180);
}

function gotPoses(results) {
  poses = results || [];
}

function gotFaces(results) {
  faces = results || [];
// Call back funtion, that always makes sure it returns array
}

// Generated by ChatGPT5, 2025-11-13
function getBodyKeypoint(index, personIndex = 0) {
  if (!poses.length || !poses[personIndex]) return null;
  const kps = poses[personIndex].keypoints;
  return (kps && kps[index]) ? kps[index] : null;
}

function getFaceKeypoint(index, faceIndex = 0) {
  if (!faces.length || !faces[faceIndex]) return null;
  const kps = faces[faceIndex].keypoints;
  return (kps && kps[index]) ? kps[index] : null;
}

// Visual helper: pulsing ring at nose
function drawPulsingTarget(x, y, c) {
  const pulseValue = (sin(frameCount / 8) + 1) * 0.5;
  const targetSize = map(pulseValue, 0, 1, 20, 40);
  const alpha = map(pulseValue, 0, 1, 100, 255);

  noFill();
  stroke(red(c), green(c), blue(c), alpha);
  strokeWeight(2);
  circle(x, y, targetSize * 2);

  fill(red(c), green(c), blue(c), alpha * 0.5);
  noStroke();
  circle(x, y, targetSize);

  textAlign(CENTER);
  textSize(14 + pulseValue * 4);
  fill(red(c), green(c), blue(c), alpha);
  text("NOSE", x, y - targetSize - 8);
}

// Serial initialization and UI
function initializeSerial() {
  try {
    port = createSerial();
    const usedPorts =
      (typeof usedSerialPorts === "function") ? usedSerialPorts() : [];
    if (usedPorts && usedPorts.length > 0) {
      port.open(usedPorts[0], 57600);
    }
  } catch (e) {
    port = null;
  }
}

function createConnectButton() {
  connectBtn = createButton("Connect to Arduino");
  connectBtn.position(10, height - 40);
  connectBtn.mousePressed(connectBtnClick);
}

function updateConnectButton() {
  if (!connectBtn) return;
  if (!port || typeof port.opened !== "function" || !port.opened()) {
    connectBtn.html("Connect to Arduino");
  } else {
    connectBtn.html("Disconnect");
  }
}

function connectBtnClick() {
  if (!port) {
    initializeSerial();
  }
  if (!port) return;

  if (!port.opened || !port.opened()) {
    try {
      port.open("Arduino", 57600);
    } catch (e) {}
  } else {
    try {
      port.close();
    } catch (e) {}
  }
}

function keyPressed() {
  if (key === ' ') {
    showVideo = !showVideo;
  }
}

function windowResized() {
  resizeCanvas(windowWidth, windowHeight);
}
